#!/usr/bin/env python3
"""
FlatQuant ç²¾åº¦æµ‹è¯•æ¨¡å—

ä¸“é—¨ç”¨äºæµ‹è¯•CUDAå®ç°ä¸PyTorchå‚è€ƒå®ç°çš„ç²¾åº¦ä¸€è‡´æ€§ã€‚
"""

import torch
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), 'src'))

from quant_ops import (
    flatquant_dynamic_quantize, 
    dequantize_int4, 
    get_decompose_dim,
    CUDA_AVAILABLE
)


def test_basic_accuracy():
    """åŸºç¡€ç²¾åº¦æµ‹è¯•"""
    print("=" * 60)
    print("åŸºç¡€ç²¾åº¦æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•å‚æ•°
    batch_tokens = 128
    features = 4096
    M, N = get_decompose_dim(features)
    
    print(f"æµ‹è¯•é…ç½®: batch_tokens={batch_tokens}, features={features}, M={M}, N={N}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    torch.manual_seed(42)
    input_fp16 = torch.randn(batch_tokens, features, dtype=torch.float16, device='cuda')
    left_trans = torch.randn(M, M, dtype=torch.float16, device='cuda')
    right_trans = torch.randn(N, N, dtype=torch.float16, device='cuda')
    clip_ratio = 1.0
    
    # PyTorch å®ç°
    print("\næµ‹è¯• PyTorch å‚è€ƒå®ç°...")
    torch_quant, torch_scales = flatquant_dynamic_quantize(
        input_fp16, left_trans, right_trans, clip_ratio, pack_int32=False, use_cuda=False
    )
    torch_dequant = dequantize_int4(torch_quant, torch_scales, packed_int32=False)
    
    if not CUDA_AVAILABLE:
        print("CUDA å®ç°ä¸å¯ç”¨ï¼Œè·³è¿‡ç²¾åº¦å¯¹æ¯”")
        return True
    
    # CUDA å®ç°
    print("æµ‹è¯• CUDA åŠ é€Ÿå®ç°...")
    try:
        cuda_quant, cuda_scales = flatquant_dynamic_quantize(
            input_fp16, left_trans, right_trans, clip_ratio, pack_int32=False, use_cuda=True
        )
        cuda_dequant = dequantize_int4(cuda_quant, cuda_scales, packed_int32=False)
        
        # ç²¾åº¦å¯¹æ¯”
        print("\nç²¾åº¦å¯¹æ¯”ç»“æœ:")
        quant_diff = (cuda_quant.float() - torch_quant.float()).abs()
        scale_diff = (cuda_scales - torch_scales).abs()
        dequant_diff = (cuda_dequant - torch_dequant).abs()
        
        print(f"é‡åŒ–ç»“æœæœ€å¤§å·®å¼‚: {quant_diff.max().item():.6f}")
        print(f"é‡åŒ–ç»“æœå¹³å‡å·®å¼‚: {quant_diff.mean().item():.6f}")
        print(f"Scale æœ€å¤§å·®å¼‚: {scale_diff.max().item():.6f}")
        print(f"Scale å¹³å‡å·®å¼‚: {scale_diff.mean().item():.6f}")
        print(f"åé‡åŒ–ç»“æœæœ€å¤§å·®å¼‚: {dequant_diff.max().item():.6f}")
        print(f"åé‡åŒ–ç»“æœå¹³å‡å·®å¼‚: {dequant_diff.mean().item():.6f}")
        print(f"åé‡åŒ–ç»“æœç›¸å¯¹è¯¯å·®: {(dequant_diff / torch_dequant.abs().clamp(min=1e-8)).mean().item():.6f}")
        
        # åˆ¤æ–­ç²¾åº¦æ˜¯å¦å¯æ¥å—
        if quant_diff.max() <= 2 and scale_diff.max() <= 1e-4:
            print("âœ… ç²¾åº¦æµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âŒ ç²¾åº¦æµ‹è¯•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"CUDA å®ç°æ‰§è¡Œå¤±è´¥: {e}")
        return False


def test_packing_accuracy():
    """æµ‹è¯•æ‰“åŒ…åŠŸèƒ½ç²¾åº¦"""
    print("\n" + "=" * 60)
    print("Int32 æ‰“åŒ…åŠŸèƒ½ç²¾åº¦æµ‹è¯•")
    print("=" * 60)
    
    if not CUDA_AVAILABLE:
        print("CUDA å®ç°ä¸å¯ç”¨ï¼Œè·³è¿‡æ‰“åŒ…æµ‹è¯•")
        return True
    
    batch_tokens, features = 128, 4096
    torch.manual_seed(42)
    M, N = get_decompose_dim(features)
    input_tensor = torch.randn(batch_tokens, features, dtype=torch.float16, device='cuda')
    left_trans = torch.randn(M, M, dtype=torch.float16, device='cuda') 
    right_trans = torch.randn(N, N, dtype=torch.float16, device='cuda')
    clip_ratio = 1.0
    
    try:
        # æµ‹è¯•ä¸æ‰“åŒ…
        quant_int8, scales_int8 = flatquant_dynamic_quantize(
            input_tensor, left_trans, right_trans, clip_ratio, pack_int32=False, use_cuda=True
        )
        dequant_int8 = dequantize_int4(quant_int8, scales_int8, packed_int32=False)
        
        # æµ‹è¯•æ‰“åŒ…
        quant_int32, scales_int32 = flatquant_dynamic_quantize(
            input_tensor, left_trans, right_trans, clip_ratio, pack_int32=True, use_cuda=True
        )
        dequant_int32 = dequantize_int4(quant_int32, scales_int32, packed_int32=True)
        
        # å¯¹æ¯”ä¸¤ç§æ–¹å¼çš„ç»“æœ
        dequant_diff = (dequant_int8 - dequant_int32).abs()
        scale_diff = (scales_int8 - scales_int32).abs()
        
        print(f"æ‰“åŒ…å‰ååé‡åŒ–ç»“æœå·®å¼‚ - æœ€å¤§: {dequant_diff.max().item():.6f}, å¹³å‡: {dequant_diff.mean().item():.6f}")
        print(f"æ‰“åŒ…å‰åScaleå€¼å·®å¼‚ - æœ€å¤§: {scale_diff.max().item():.6f}, å¹³å‡: {scale_diff.mean().item():.6f}")
        
        # æ£€æŸ¥å­˜å‚¨æ•ˆç‡
        memory_int8 = quant_int8.numel() * quant_int8.element_size()
        memory_int32 = quant_int32.numel() * quant_int32.element_size()
        compression_ratio = memory_int8 / memory_int32
        
        print(f"å­˜å‚¨æ•ˆç‡: int8={memory_int8}bytes, int32={memory_int32}bytes, å‹ç¼©æ¯”={compression_ratio:.2f}x")
        
        if dequant_diff.max() <= 1e-6 and compression_ratio >= 1.9:  # ç†è®ºä¸Šåº”è¯¥æ˜¯2xå‹ç¼©
            print("âœ… æ‰“åŒ…åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âŒ æ‰“åŒ…åŠŸèƒ½æµ‹è¯•å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"æ‰“åŒ…åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰ç²¾åº¦æµ‹è¯•"""
    print("FlatQuant ç²¾åº¦æµ‹è¯•")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    print(f"CUDA ç®—å­å¯ç”¨: {CUDA_AVAILABLE}")
    
    if not torch.cuda.is_available():
        print("é”™è¯¯: CUDA ä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡Œæµ‹è¯•")
        return False
    
    # è¿è¡Œæµ‹è¯•
    all_passed = True
    all_passed &= test_basic_accuracy()
    all_passed &= test_packing_accuracy()
    
    print("\n" + "=" * 60)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰ç²¾åº¦æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
    print("=" * 60)
    
    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 